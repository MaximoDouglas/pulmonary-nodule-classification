{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "002_features_extraction_and_fusion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YniqkXDJ5YW7",
        "colab_type": "text"
      },
      "source": [
        "# Env Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlQCyABiAiKc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "21051812-d914-40ab-a3fa-df23fdf31c8c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive', force_remount=True)\n",
        "!pip install keras_metrics"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at drive\n",
            "Collecting keras_metrics\n",
            "  Downloading https://files.pythonhosted.org/packages/32/c9/a87420da8e73de944e63a8e9cdcfb1f03ca31a7c4cdcdbd45d2cdf13275a/keras_metrics-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.6/dist-packages (from keras_metrics) (2.4.3)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.18.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras>=2.1.5->keras_metrics) (1.15.0)\n",
            "Installing collected packages: keras-metrics\n",
            "Successfully installed keras-metrics-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpsuDVzrYKVD",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN_yFwwqYKVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import interp\n",
        "from hyperopt import Trials, STATUS_OK, tpe\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "from keras import backend as K\n",
        "from keras import optimizers\n",
        "from keras.layers import Conv3D, MaxPool3D, Flatten, Dense, Dropout, Input, concatenate\n",
        "from keras.losses import binary_crossentropy\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.utils.vis_utils import plot_model, model_to_dot\n",
        "import keras_metrics as km\n",
        "import math\n",
        "import itertools\n",
        "import re\n",
        "import os\n",
        "import imageio\n",
        "from scipy.ndimage import rotate\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "'''Data settings'''\n",
        "rotations_of_benignant = 10\n",
        "rotations_of_malignant = 10\n",
        "slices_per_nodule      = 5\n",
        "base_dir               = \"/content/drive/My Drive/masters/Research/data/\"\n",
        "images_dir             = base_dir + \"images/solid-nodules/\"\n",
        "features_path          = base_dir + \"features/solidNodules.csv\"\n",
        "features_names         = pd.read_csv(features_path).columns[2:73]\n",
        "destination_folder     = base_dir + \"convolutional_features/deep_features_with_radiomics/\"\n",
        "images_resolution      = 64\n",
        "\n",
        "'''Train/Validation settings'''\n",
        "LEARNING_RATE = 0.0001\n",
        "FOLDS         = 10\n",
        "\n",
        "'''Model params'''\n",
        "convolucional_layer_units = 96\n",
        "dense_layer_units_1       = 64\n",
        "dense_layer_units_2       = 24\n",
        "dropout_layer_1           = 0.241\n",
        "dropout_layer_2           = 0.236\n",
        "input_shape               = (64, 64, 5, 1)\n",
        "\n",
        "'''Radiomic Feature Sets'''\n",
        "shape_features = [14, 15, 16, 17, 18, 19, 20, 21]\n",
        "\n",
        "intensity_features = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
        "\n",
        "texture_features = [23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, \n",
        "                    42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58]\n",
        "\n",
        "edge_sharpness_features = [59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70]\n",
        "\n",
        "optimized_features = [0, 1, 2, 4, 7, 9, 10, 16, 17, 20, 24, 26, 27, 28, 29, 30, 31, 33, 36, 38, \n",
        "                      39, 42, 44, 45, 46, 48, 51, 52, 54, 55, 57, 58, 60, 61, 63, 65, 69, 70]\n",
        "\n",
        "all_features_set = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n",
        "                    21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, \n",
        "                    39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, \n",
        "                    57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70]\n",
        "\n",
        "'''Iterables'''\n",
        "result_dfs = {}\n",
        "layers     = ['dense1', 'dense2']\n",
        "feat_sets  = {'shape_features': shape_features, \n",
        "              'intensity_features': intensity_features,\n",
        "              'texture_features': texture_features,\n",
        "              'edge_sharpness_features': edge_sharpness_features,\n",
        "              'optimized_features': optimized_features, \n",
        "              'all_features_set': all_features_set, \n",
        "              'none': []\n",
        "              }"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ74XWbH5QZj",
        "colab_type": "text"
      },
      "source": [
        "## Normalize getting the first slices\n",
        "Function that normalize getting the first slices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBBPHYNH2YU6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_first(nodules, n_slices):\n",
        "    '''Normalizes the nodule slices number:\n",
        "    - A nodule with less than n slices is completed with black slices\n",
        "    - A nodule with more than n slices have its n first slices selected'''\n",
        "    \n",
        "    normalized_slices = []\n",
        "\n",
        "    for nodule in nodules:\n",
        "        new_nodule = []\n",
        "\n",
        "        if len(nodule) <= n_slices:\n",
        "                for slice in nodule:\n",
        "                    new_nodule.append(slice)\n",
        "                for i in range(n_slices - len(nodule)):\n",
        "                    new_nodule.append(np.zeros((images_resolution, images_resolution)))\n",
        "        elif len(nodule) > n_slices:\n",
        "            for i in range(0, n_slices):\n",
        "                new_nodule.append(nodule[i])\n",
        "        normalized_slices.append(new_nodule)\n",
        "    return normalized_slices"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR3WpFAJ4-dR",
        "colab_type": "text"
      },
      "source": [
        "## Data augmentation\n",
        "Function that augment the data by rotating the slices of the nodules\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "580I7sTv3Ya3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rotate_slices(nodules, f, times, mode='constant'):\n",
        "    ''' Rotates a list of images n times'''\n",
        "    \n",
        "    rotated = nodules\n",
        "    angle = 360/times\n",
        "    rep_feat = f\n",
        "\n",
        "    for i in range(1, times):\n",
        "        temp = rotate(nodules, i*angle, (1, 2), reshape=False, mode=mode)\n",
        "        rotated     = np.concatenate([rotated, temp])\n",
        "        rep_feat    = np.concatenate([rep_feat, f])\n",
        "\n",
        "    return rotated, rep_feat"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bv9yE5y5HhX",
        "colab_type": "text"
      },
      "source": [
        "## Read images\n",
        "Function to read images from files and returns a list of numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFArGLMh3MDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_images(path, path_features):\n",
        "    '''Reads the images files in our file structure and mounts an array\n",
        "    Parameters:\n",
        "        path (string): path to the nodules folders\n",
        "        path_features (string): path to the features .csv\n",
        "    Returns:\n",
        "        list: list of nodules with slices as Numpy Arrays\n",
        "        features: list of features corresponding to the nodules on list'''\n",
        "    \n",
        "    df = pd.read_csv(path_features)\n",
        "    \n",
        "    scaler = MinMaxScaler(copy=False)\n",
        "    df[df.columns[2:73]] = scaler.fit_transform(df[df.columns[2:73]])\n",
        "    \n",
        "    allFeatures = df.values\n",
        "\n",
        "    lista    = []\n",
        "    features = []\n",
        "\n",
        "    for _, dirs, _ in os.walk(path):\n",
        "        for dirname in sorted(dirs, key=str.lower):\n",
        "            for _, dirs1, _ in os.walk(path + \"/\" + dirname):\n",
        "                for dirname1 in sorted(dirs1, key=str.lower):\n",
        "                    for root2, _, files2 in os.walk(path + \"/\" + dirname + \"/\" + dirname1):\n",
        "                        slices = []\n",
        "                        files2[:] = [re.findall('\\d+', x)[0] for x in files2]\n",
        "\n",
        "                        axis         = 0 # To get the Rows indices\n",
        "                        examColumn   = 0 # Column of the csv where the exam code is\n",
        "                        noduleColumn = 1 # Column of the csv where the nodule code is\n",
        "\n",
        "                        # index of the rows that have the exam id equal to the exam id of the current nodule\n",
        "                        indExam = np.where(allFeatures[:,examColumn] == dirname)[axis]\n",
        "\n",
        "                        # index of the rows that have the nodule id equal to the id of the current nodule\n",
        "                        indNodule = np.where(allFeatures[:,noduleColumn] == dirname1)[axis]\n",
        "\n",
        "                        i = np.intersect1d(indExam,indNodule)\n",
        "\n",
        "                        # A list is returned, but there's just one value, so I used its index\n",
        "                        index = 0\n",
        "                        exam = allFeatures[i,examColumn][index]\n",
        "                        nodule = allFeatures[i,noduleColumn][index]\n",
        "\n",
        "                        '''Verify if there's more than one index for each nodule\n",
        "                        and if there's divergence between the nodule location and the\n",
        "                        csv values'''\n",
        "\n",
        "                        if((len(i) > 1) or (str(exam) != str(dirname)) or (str(nodule) != str(dirname1))):\n",
        "                            print(\"Features error!\")\n",
        "                        else:\n",
        "                            '''Transform the list of index with just one value in a\n",
        "                            primitive value to use as index to save the features values'''\n",
        "                            i = i[0]\n",
        "\n",
        "                        for f in sorted(files2, key=float):\n",
        "                            img = imageio.imread(root2 + \"/\" + f + \".png\", as_gray=True)\n",
        "                            slices.append(img)\n",
        "\n",
        "                        lista.append(slices)\n",
        "                        \n",
        "                        features.append(allFeatures[i,2:73].tolist())\n",
        "\n",
        "    return lista, features"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epU6Ruv140uh",
        "colab_type": "text"
      },
      "source": [
        "## My Kfold\n",
        "k_folder made to get balanced data between benigno and maligno"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUUW5lpf3f28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_kfold(ben, mal, f_ben, f_mal, n_splits, ben_rot, mal_rot):\n",
        "    kf = KFold(n_splits)\n",
        "    \n",
        "    f_mal_train, f_mal_test = [], []\n",
        "    mal_train, mal_test = [], []\n",
        "    for train_index, test_index in kf.split(mal):\n",
        "        mal_train.append([mal[index] for index in train_index])\n",
        "        f_mal_train.append([f_mal[index] for index in train_index])\n",
        "\n",
        "        mal_test.append([mal[index] for index in test_index])\n",
        "        f_mal_test.append([f_mal[index] for index in test_index])\n",
        "\n",
        "    ben_train, ben_test = [], []\n",
        "    f_ben_train, f_ben_test = [], []\n",
        "    \n",
        "    # percorro o mal_test para que os folds de test tenham o mesmo número de itens\n",
        "    for (train_index, test_index), mal_te, mal_tr in zip(kf.split(ben), mal_test, mal_train):\n",
        "        \n",
        "        sample = np.random.choice(test_index, len(mal_te), replace=False)\n",
        "        sample_ = np.setdiff1d(test_index, sample)\n",
        "\n",
        "        ben_train_ind = np.concatenate((train_index, sample_))\n",
        "\n",
        "        '''This line guarantees that the ben and mal train batches are the same size'''\n",
        "        ben_train_ind = np.random.choice(ben_train_ind, len(mal_tr), replace=False)\n",
        "\n",
        "        ben_train.append([ben[index] for index in ben_train_ind])\n",
        "        f_ben_train.append([f_ben[index] for index in ben_train_ind])\n",
        "        \n",
        "        ben_test.append([ben[index] for index in sample])\n",
        "        f_ben_test.append([f_ben[index] for index in sample])\n",
        "\n",
        "    X_test, Y_test = [], []\n",
        "    for b, m in zip(ben_test, mal_test):\n",
        "        X_test.append(np.concatenate((b, m), 0))\n",
        "\n",
        "        y_test = len(b) * [0] + len(m) * [1]\n",
        "        Y_test.append(np.array(y_test))\n",
        "\n",
        "    f_test = []\n",
        "    for f_b, f_m in zip(f_ben_test, f_mal_test):\n",
        "        \n",
        "        f_test.append(np.concatenate((f_b, f_m), 0))\n",
        "\n",
        "    X_train, Y_train = [], []\n",
        "    f_train = []\n",
        "    for i in tqdm(range(n_splits)):\n",
        "        print(\"INDEX: \", i)\n",
        "        print(\"ben_train: \", len(ben_train[i]))\n",
        "        print(\"mal_train: \", len(mal_train[i]))\n",
        "        print(\"ben_test: \",  len(ben_test[i]))\n",
        "        print(\"mal_test: \",  len(mal_test[i]))\n",
        "        \n",
        "        b, m = ben_train[i], mal_train[i]\n",
        "        f_b_train, f_m_train = f_ben_train[i], f_mal_train[i]\n",
        "\n",
        "        b, f_b_train = rotate_slices(nodules=b, f=f_b_train, times=ben_rot)\n",
        "        m, f_m_train = rotate_slices(nodules=m, f=f_m_train, times=mal_rot)\n",
        "\n",
        "        X_train.append(np.concatenate((b, m), 0))\n",
        "        f_train.append(np.concatenate((f_b_train, f_m_train), 0))\n",
        "\n",
        "        y_train = len(b) * [0] + len(m) * [1]\n",
        "        Y_train.append(np.array(y_train))\n",
        "\n",
        "    return X_train, X_test, f_train, f_test, Y_train, Y_test"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBFxM-wS4nVo",
        "colab_type": "text"
      },
      "source": [
        "## Get folds\n",
        "Function that is called to get the folds of the cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyB7azPc3srM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_folds(basedir, n_slices, features=None):\n",
        "    ben_dir = basedir + \"benigno\"\n",
        "    mal_dir = basedir + \"maligno\"\n",
        "\n",
        "    ben, f_ben = read_images(ben_dir, features)\n",
        "    mal, f_mal = read_images(mal_dir, features)\n",
        "\n",
        "    ben = normalize_first(ben, n_slices)\n",
        "    mal = normalize_first(mal, n_slices)\n",
        "\n",
        "    ben = np.array(ben).reshape(len(ben), n_slices, images_resolution, images_resolution, 1)\n",
        "    mal = np.array(mal).reshape(len(mal), n_slices, images_resolution, images_resolution, 1)\n",
        "\n",
        "    ben = np.moveaxis(ben, 1, 3)\n",
        "    mal = np.moveaxis(mal, 1, 3)\n",
        "\n",
        "    ben_zip = list(zip(ben, f_ben))\n",
        "    np.random.shuffle(ben_zip)\n",
        "    ben, f_ben = zip(*ben_zip)\n",
        "\n",
        "    mal_zip = list(zip(mal, f_mal))\n",
        "    np.random.shuffle(mal_zip)\n",
        "    mal, f_mal = zip(*mal_zip)\n",
        "\n",
        "    X_train, X_test, f_train, f_test, Y_train, Y_test = my_kfold(ben, mal, f_ben, \n",
        "                                                                 f_mal, FOLDS, \n",
        "                                                                 rotations_of_benignant, \n",
        "                                                                 rotations_of_malignant)\n",
        "\n",
        "    return X_train, X_test, f_train, f_test, Y_train, Y_test"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDubOPP3YKV4",
        "colab_type": "text"
      },
      "source": [
        "# Valition code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FLvGJNFYKV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model():\n",
        "    input_layer = Input(input_shape)\n",
        "    conv_layer1 = Conv3D(filters=convolucional_layer_units, kernel_size=(3, 3, 3), \n",
        "                        activation='relu')(input_layer)\n",
        "    pooling_layer1 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer1)\n",
        "    \n",
        "    flatten_layer  = Flatten(name='flatten')(pooling_layer1)\n",
        "    \n",
        "    dense_layer1 = Dense(units=dense_layer_units_1, activation='relu', name='dense1')(flatten_layer)\n",
        "    dense_layer1 = Dropout(dropout_layer_1)(dense_layer1)\n",
        "\n",
        "    dense_layer2 = Dense(units=dense_layer_units_2, activation='relu', name='dense2')(dense_layer1)\n",
        "    dense_layer2 = Dropout(dropout_layer_2)(dense_layer2)\n",
        "\n",
        "    output_layer = Dense(units=1, activation='sigmoid')(dense_layer2)\n",
        "    \n",
        "    model = []\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "      \n",
        "    opt = optimizers.RMSprop(lr=LEARNING_RATE)\n",
        "\n",
        "    model.compile(loss=binary_crossentropy, optimizer=opt, \n",
        "    metrics=['accuracy', km.binary_true_positive(), km.binary_true_negative(), \n",
        "                km.binary_false_positive(), km.binary_false_negative(), \n",
        "                km.binary_f1_score()])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxfPLx_XYKWG",
        "colab_type": "text"
      },
      "source": [
        "## Cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5q4RtMfYKWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = time.time()\n",
        "X_train_, X_test_, f_train_, f_test_, Y_train_, Y_test_ = get_folds(basedir=images_dir, n_slices=slices_per_nodule, features=features_path)\n",
        "\n",
        "i = 0\n",
        "for X_train, X_test, f_train, f_test, Y_train, Y_test in zip(X_train_, X_test_, f_train_, f_test_, Y_train_, Y_test_):\n",
        "  i += 1\n",
        "  print(\"ITERATION: \" + str(i))\n",
        "  \n",
        "  model = get_model()\n",
        "  model.fit(X_train, Y_train, batch_size=128, epochs=10, verbose=0)\n",
        "\n",
        "  for layer in layers:\n",
        "    intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer(layer).output)\n",
        "    output                   = intermediate_layer_model.predict(X_test)\n",
        "    \n",
        "    for feature_set in feat_sets:\n",
        "      deep_features_df = pd.DataFrame(output)\n",
        "      file_name        = destination_folder + (layer) + \"_\" + feature_set + \".csv\"\n",
        "      \n",
        "      print(file_name)\n",
        "      features_selected_set = feat_sets[feature_set]\n",
        "      \n",
        "      if (feature_set != 'none'):\n",
        "        features_selected_set_df = pd.DataFrame(f_test[:,features_selected_set], columns=features_names[features_selected_set])\n",
        "        concat_features_df       = pd.concat([deep_features_df, features_selected_set_df], axis=1, sort=False)\n",
        "        deep_features_df         = concat_features_df\n",
        "      \n",
        "      deep_features_df['class'] = Y_test\n",
        "      \n",
        "      if (file_name not in result_dfs):\n",
        "        result_dfs[file_name] = deep_features_df\n",
        "      else:\n",
        "        result_dfs[file_name] = pd.concat([result_dfs[file_name], deep_features_df])\n",
        "\n",
        "for df in result_dfs:\n",
        "  result_dfs[df].to_csv(df, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}