{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "validation_5_first.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "YniqkXDJ5YW7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Env Configuration"
      ]
    },
    {
      "metadata": {
        "id": "OlQCyABiAiKc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lboQyY5-BASc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install hyperas\n",
        "!pip install keras_metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jpsuDVzrYKVD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ]
    },
    {
      "metadata": {
        "id": "DN_yFwwqYKVF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import time\n",
        "\n",
        "import gc\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from scipy import interp\n",
        "\n",
        "from hyperopt import Trials, STATUS_OK, tpe\n",
        "from hyperas import optim\n",
        "from hyperas.distributions import choice, uniform\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "\n",
        "from keras import backend as K\n",
        "from keras import optimizers\n",
        "from keras.layers import Conv3D, MaxPool3D, Flatten, Dense, Dropout, Input\n",
        "from keras.losses import binary_crossentropy\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.utils.vis_utils import plot_model, model_to_dot\n",
        "\n",
        "import keras_metrics as km\n",
        "\n",
        "import math\n",
        "import itertools\n",
        "import re\n",
        "import os\n",
        "import imageio\n",
        "from scipy.ndimage import rotate\n",
        "from sklearn.model_selection import KFold\n",
        "from tqdm import tqdm\n",
        "import shutil"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rFLHOBV9PLJs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## GPU configuration"
      ]
    },
    {
      "metadata": {
        "id": "KhTqgvoNYKVP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.backend.tensorflow_backend import set_session\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 1\n",
        "set_session(tf.Session(config=config))\n",
        "session = tf.Session(config=config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZUQwTiVf2Vve",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Important functions\n",
        "The following blocks of code were made to be used in the process of validation of the model found by the optmizer of the refered strategy (optimizer_5_first)."
      ]
    },
    {
      "metadata": {
        "id": "wQ74XWbH5QZj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Normalize getting the first slices\n",
        "Function that normalize getting the first slices"
      ]
    },
    {
      "metadata": {
        "id": "TBBPHYNH2YU6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def normalize_first(nodules, n_slices, repeat=False):\n",
        "    '''Normalizes the nodule slices number:\n",
        "    - A nodule with less than n slices is completed with black slices\n",
        "    - A nodule with more than n slices have its n first slices selected\n",
        "    '''\n",
        "    normalized_slices = []\n",
        "\n",
        "    for nodule in nodules:\n",
        "        new_nodule = []\n",
        "\n",
        "        if repeat:\n",
        "            times = math.ceil(n_slices/len(nodule))\n",
        "            nodule = list(itertools.chain.from_iterable(itertools.repeat(x, times) for x in nodule))\n",
        "\n",
        "        if len(nodule) <= n_slices:\n",
        "                for slice in nodule:\n",
        "                    new_nodule.append(slice)\n",
        "                for i in range(n_slices - len(nodule)):\n",
        "                    new_nodule.append(np.zeros((RES, RES)))\n",
        "        elif len(nodule) > n_slices:\n",
        "            for i in range(0, n_slices):\n",
        "                new_nodule.append(nodule[i])\n",
        "        normalized_slices.append(new_nodule)\n",
        "    return normalized_slices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6bv9yE5y5HhX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Read images\n",
        "Function to read images from files and returns a list of numpy"
      ]
    },
    {
      "metadata": {
        "id": "yFArGLMh3MDG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_images(path, category):\n",
        "    '''Reads the images files in our file structure and mounts an array\n",
        "    Parameters:\n",
        "        path (string): path to the nodules folders\n",
        "        category (string): benigno or maligno\n",
        "    Returns:\n",
        "        list: list of nodules with slices as Numpy Arrays\n",
        "    '''\n",
        "    lista = []\n",
        "\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for dirname in sorted(dirs, key=str.lower):\n",
        "            for root1, dirs1, files1 in os.walk(path + \"/\" + dirname):\n",
        "                for dirname1 in sorted(dirs1, key=str.lower):\n",
        "                    for root2, dirs2, files2 in os.walk(path + \"/\" + dirname + \"/\" + dirname1):\n",
        "                        slices = []\n",
        "                        files2[:] = [re.findall('\\d+', x)[0] for x in files2]\n",
        "\n",
        "                        for f in sorted(files2, key=float):\n",
        "                            img = imageio.imread(root2 + \"/\" + f + \".png\", as_gray=True)\n",
        "                            slices.append(img)\n",
        "\n",
        "                        lista.append(slices)\n",
        "    return lista"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZR3WpFAJ4-dR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data augmentation\n",
        "Function that augment the data rotating slices"
      ]
    },
    {
      "metadata": {
        "id": "580I7sTv3Ya3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def rotate_slices(slices, times, mode='constant'):\n",
        "    ''' Rotates a list of images n times'''\n",
        "    rotated = slices\n",
        "    angle = 360/times\n",
        "    for i in range(1, times):\n",
        "        temp = rotate(slices, i*angle, (1, 2), reshape=False, mode = mode)\n",
        "        rotated = np.concatenate([rotated, temp])\n",
        "    return rotated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "epU6Ruv140uh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## My Kfold\n",
        "k_folder made to get balanced data between benigno and maligno"
      ]
    },
    {
      "metadata": {
        "id": "IUUW5lpf3f28",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def my_kfold(ben, mal, n_splits, ben_rot, mal_rot):\n",
        "    kf = KFold(n_splits)\n",
        "\n",
        "    mal_train, mal_test = [], []\n",
        "    for train_index, test_index in kf.split(mal):\n",
        "        mal_train.append(mal[train_index])\n",
        "        mal_test.append(mal[test_index])\n",
        "\n",
        "    ben_train, ben_test = [], []\n",
        "    # percorro o mal_test para que os folds de test tenham o mesmo n√∫mero de itens\n",
        "    for (train_index, test_index), mal in zip(kf.split(ben), mal_test):\n",
        "        sample = np.random.choice(test_index, len(mal), replace=False)\n",
        "        sample_ = np.setdiff1d(test_index, sample)\n",
        "\n",
        "        ben_train.append(ben[np.concatenate((train_index, sample_))])\n",
        "        ben_test.append(ben[sample])\n",
        "\n",
        "    X_test, Y_test = [], []\n",
        "    for b, m in zip(ben_test, mal_test):\n",
        "        X_test.append(np.concatenate((b, m), 0))\n",
        "\n",
        "        y_test = len(b) * [0] + len(m) * [1]\n",
        "        Y_test.append(np.array(y_test))\n",
        "        #Y_test.append(to_categorical(y_test))\n",
        "\n",
        "    X_train, Y_train = [], []\n",
        "    for i in tqdm(range(n_splits)):\n",
        "        b, m = ben_train[i], mal_train[i]\n",
        "\n",
        "        b = rotate_slices(b, ben_rot)\n",
        "        m = rotate_slices(m, mal_rot)\n",
        "\n",
        "        X_train.append(np.concatenate((b, m), 0))\n",
        "\n",
        "        y_train = len(b) * [0] + len(m) * [1]\n",
        "        Y_train.append(np.array(y_train))\n",
        "        #Y_train.append(to_categorical(y_train))\n",
        "\n",
        "    return X_train, X_test, Y_train, Y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jBFxM-wS4nVo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Get folds\n",
        "Function that is called to get the folds of the cross validation"
      ]
    },
    {
      "metadata": {
        "id": "ZyB7azPc3srM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_folds(basedir, n_slices, strategy='first', repeat=False):\n",
        "    ben_dir = basedir + \"benigno/\"\n",
        "    mal_dir = basedir + \"maligno/\"\n",
        "\n",
        "    ben = read_images(ben_dir, \"benigno\")\n",
        "    mal = read_images(mal_dir, \"maligno\")\n",
        "\n",
        "    if strategy == 'first':\n",
        "        ben = normalize_first(ben, n_slices, repeat)\n",
        "        mal = normalize_first(mal, n_slices, repeat)\n",
        "    elif strategy == 'balanced':\n",
        "        ben = normalize_balanced(ben, n_slices, repeat)\n",
        "        mal = normalize_balanced(mal, n_slices, repeat)\n",
        "\n",
        "    ben = np.concatenate(ben).reshape(len(ben), n_slices, RES, RES, 1)\n",
        "    mal = np.concatenate(mal).reshape(len(mal), n_slices, RES, RES, 1)\n",
        "\n",
        "    ben = np.moveaxis(ben, 1, 3)\n",
        "    mal = np.moveaxis(mal, 1, 3)\n",
        "\n",
        "    np.random.shuffle(ben)\n",
        "    np.random.shuffle(mal)\n",
        "\n",
        "    X_train, X_test, Y_train, Y_test = my_kfold(ben, mal, 10, 5, 13)\n",
        "\n",
        "    return X_train, X_test, Y_train, Y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VDubOPP3YKV4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Valition code\n"
      ]
    },
    {
      "metadata": {
        "id": "JpeEFMx9YKVU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def data():\n",
        "  prefix = \"/content/drive/My Drive/Pesquisa - Dicom images/data\"\n",
        "  X_train = np.load(prefix + \"/nps/solid-nodules/data-5-first/X_train.npy\")\n",
        "  X_test = np.load(prefix + \"/nps/solid-nodules/data-5-first/X_test.npy\")\n",
        "  Y_train = np.load(prefix + \"/nps/solid-nodules/data-5-first/Y_train.npy\")\n",
        "  Y_test = np.load(prefix + \"/nps/solid-nodules/data-5-first/Y_test.npy\")\n",
        "  \n",
        "  return X_train, Y_train, X_test, Y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2FLvGJNFYKV5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "c1 = 32\n",
        "d1 = 96\n",
        "d2 = 16\n",
        "drop1 = 0.07176927609418894\n",
        "drop2 = 0.2837032463233716\n",
        "\n",
        "X, Y, x, y = data()\n",
        "\n",
        "def get_model():\n",
        "    \n",
        "    K.clear_session()\n",
        "    gc.collect()\n",
        "    \n",
        "    input_layer = Input(X.shape[1:5])\n",
        "\n",
        "    conv_layer1 = Conv3D(filters=c1, kernel_size=(3, 3, 3), activation='relu')(input_layer)\n",
        "    pooling_layer1 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer1)\n",
        "\n",
        "    flatten_layer = Flatten()(pooling_layer1)\n",
        "\n",
        "    dense_layer1 = Dense(units=d1, activation='relu')(flatten_layer)\n",
        "    dense_layer1 = Dropout(drop1)(dense_layer1)\n",
        "\n",
        "    dense_layer2 = Dense(units=d2, activation='relu')(dense_layer1)\n",
        "    dense_layer2 = Dropout(drop2)(dense_layer2)\n",
        "\n",
        "    output_layer = Dense(units=1, activation='sigmoid')(dense_layer2)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    opt = optimizers.RMSprop(lr=0.0001)\n",
        "\n",
        "    model.compile(loss=binary_crossentropy, optimizer=opt, metrics=['accuracy', km.binary_true_positive(), km.binary_true_negative(), km.binary_false_positive(), km.binary_false_negative(), km.binary_f1_score()])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OIuBgfYYYKV8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = get_model()\n",
        "model_json = model.to_json()\n",
        "with open(\"/content/drive/My Drive/Pesquisa - Dicom images/notebooks/validation_5_first.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dxfPLx_XYKWG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Cross-validation"
      ]
    },
    {
      "metadata": {
        "id": "A5q4RtMfYKWO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "N_SLICES = 5\n",
        "\n",
        "metrics = {'acc': [], 'spec': [], 'sens': [], 'f1_score': [], 'auc': []}\n",
        "\n",
        "def sensitivity(tp, fn):\n",
        "    return tp/(tp+fn)\n",
        "\n",
        "def specificity(tn, fp):\n",
        "    return tn/(tn+fp)\n",
        "\n",
        "tprs = []\n",
        "base_fpr = np.linspace(0, 1, 101)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for i in range(1):\n",
        "    m = {'acc': [], 'spec': [], 'sens': [], 'f1_score': [], 'auc': []}\n",
        "    \n",
        "    X_train_, X_test_, Y_train_, Y_test_= get_folds(\"/content/drive/My Drive/Pesquisa - Dicom images/data/images/solid-nodules-with-attributes/\", \n",
        "                                                    N_SLICES, strategy='first', repeat=False)\n",
        "    \n",
        "    for X_train, X_test, Y_train, Y_test in zip(X_train_, X_test_, Y_train_, Y_test_):\n",
        "        model = get_model()\n",
        "        \n",
        "        model.fit(X_train, Y_train, batch_size=128, epochs=10, verbose=0)\n",
        "\n",
        "        scores = model.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "        tp, tn, fp, fn = scores[2], scores[3], scores[4], scores[5]\n",
        "        \n",
        "        acc = scores[1]*100\n",
        "        spec = specificity(tn, fp)*100\n",
        "        sens = sensitivity(tp, fn)*100\n",
        "        f1_score = scores[6]*100\n",
        "        \n",
        "        # AUC\n",
        "        pred = model.predict(X_test).ravel()\n",
        "        fpr, tpr, thresholds_keras = roc_curve(Y_test, pred)\n",
        "        auc_val = auc(fpr, tpr)\n",
        "        \n",
        "        tpr = interp(base_fpr, fpr, tpr)\n",
        "        tpr[0] = 0.0\n",
        "        tprs.append(tpr)\n",
        "    \n",
        "        m['acc'].append(acc)\n",
        "        m['spec'].append(spec)\n",
        "        m['sens'].append(sens)\n",
        "        m['f1_score'].append(f1_score)\n",
        "        m['auc'].append(auc_val)\n",
        "        \n",
        "        print(\"acc: %.2f%% spec: %.2f%% sens: %.2f%% f1: %.2f%% auc: %.2f\" % (acc, spec, sens, f1_score, auc_val))\n",
        "        \n",
        "    metrics['acc'] = metrics['acc'] + m['acc']\n",
        "    metrics['spec'] = metrics['spec'] + m['spec']\n",
        "    metrics['sens'] = metrics['sens'] + m['sens']\n",
        "    metrics['f1_score'] = metrics['f1_score'] + m['f1_score']\n",
        "    metrics['auc'] = metrics['auc'] + m['auc']\n",
        "    \n",
        "    print(\"Accuracy: %.2f%% (+/- %.2f%%)\" % (np.mean(m['acc']), np.std(m['acc'])))\n",
        "    print(\"Specificity: %.2f%% (+/- %.2f%%)\" % (np.mean(m['spec']), np.std(m['spec'])))\n",
        "    print(\"Sensitivity: %.2f%% (+/- %.2f%%)\" % (np.mean(m['sens']), np.std(m['sens'])))\n",
        "    print(\"F1-score: %.2f%% (+/- %.2f%%)\" % (np.mean(m['f1_score']), np.std(m['f1_score'])))\n",
        "    print(\"AUC: %.2f (+/- %.2f)\" % (np.mean(m['auc']), np.std(m['auc'])))\n",
        "    \n",
        "end = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pcrOI-fzYKWU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Tempo para valida√ß√£o:\", (end - start)/60, \"minutos\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JZMlPDbKYKWc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Accuracy: %.2f%% (+/- %.2f%%)\" % (np.mean(metrics['acc']), np.std(metrics['acc'])))\n",
        "print(\"Specificity: %.2f%% (+/- %.2f%%)\" % (np.mean(metrics['spec']), np.std(metrics['spec'])))\n",
        "print(\"Sensitivity: %.2f%% (+/- %.2f%%)\" % (np.mean(metrics['sens']), np.std(metrics['sens'])))\n",
        "print(\"F1-score: %.2f%% (+/- %.2f%%)\" % (np.mean(metrics['f1_score']), np.std(metrics['f1_score'])))\n",
        "print(\"AUC: %.2f (+/- %.2f)\" % (np.mean(metrics['auc']), np.std(metrics['auc'])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RafHuDD0YKWg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Curva ROC"
      ]
    },
    {
      "metadata": {
        "id": "GQqJxGAIYKWh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "tprs = np.array(tprs)\n",
        "mean_tprs = tprs.mean(axis=0)\n",
        "std = tprs.std(axis=0)\n",
        "\n",
        "tprs_upper = np.minimum(mean_tprs + std, 1)\n",
        "tprs_lower = mean_tprs - std\n",
        "\n",
        "plt.plot(base_fpr, mean_tprs, 'b')\n",
        "plt.fill_between(base_fpr, tprs_lower, tprs_upper, color='grey', alpha=0.3)\n",
        "\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([-0.01, 1.01])\n",
        "plt.ylim([-0.01, 1.01])\n",
        "plt.title(\"CNN1 - primeiras com repeti√ß√£o - 5 fatias\")\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.axes().set_aspect('equal', 'datalim')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}