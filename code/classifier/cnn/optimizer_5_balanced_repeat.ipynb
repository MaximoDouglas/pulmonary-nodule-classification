{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "optimizer_5_balanced_repeat.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "f_tVbq4yAj4n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Colab config"
      ]
    },
    {
      "metadata": {
        "id": "OlQCyABiAiKc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lboQyY5-BASc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install hyperas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jpsuDVzrYKVD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CNN 1, primeiras com repetição, 5 fatias\n",
        "\n",
        "Importando as bibliotecas necessárias."
      ]
    },
    {
      "metadata": {
        "id": "DN_yFwwqYKVF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import time\n",
        "\n",
        "import gc\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from scipy import interp\n",
        "\n",
        "from hyperopt import Trials, STATUS_OK, tpe\n",
        "from hyperas import optim\n",
        "from hyperas.distributions import choice, uniform\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "\n",
        "from keras import backend as K\n",
        "from keras import optimizers\n",
        "from keras.layers import Conv3D, MaxPool3D, Flatten, Dense, Dropout, Input\n",
        "from keras.losses import binary_crossentropy\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.utils.vis_utils import plot_model, model_to_dot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KhTqgvoNYKVP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.backend.tensorflow_backend import set_session\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 1\n",
        "set_session(tf.Session(config=config))\n",
        "session = tf.Session(config=config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4rjZxHfNYKVT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Otimização dos Hiperparâmetros\n",
        "\n",
        "Função **data()** exigida pelo Hyperas. Deve retornar os 4 vetores X_train, Y_train, X_test, Y_test.\n",
        "\n",
        "Os dados estão armazenados em 4 arquivos numpy, gerados pelo script **import_images.py**."
      ]
    },
    {
      "metadata": {
        "id": "JpeEFMx9YKVU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''5-balanced-repeat'''\n",
        "\n",
        "def data():\n",
        "  prefix = \"/content/drive/My Drive/Pesquisa - Dicom images/data\"\n",
        "  X_train = np.load(prefix + \"/nps/solid-nodules/data-5-balanced-repeat/X_train.npy\")\n",
        "  X_test = np.load(prefix + \"/nps/solid-nodules/data-5-balanced-repeat/X_test.npy\")\n",
        "  Y_train = np.load(prefix + \"/nps/solid-nodules/data-5-balanced-repeat/Y_train.npy\")\n",
        "  Y_test = np.load(prefix + \"/nps/solid-nodules/data-5-balanced-repeat/Y_test.npy\")\n",
        "  \n",
        "  return X_train, Y_train, X_test, Y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-12DCh5kYKVX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Definição do modelo a ser otimizado pelo Hyperas.\n",
        "\n",
        "O espaço de busca do modelo é o seguinte:\n",
        "* **conv_layer1**: [32, 48, 64, 96] unidades\n",
        "* **dense_layer1**: [32, 64, 128, 256] unidades\n",
        "* **dense_layer2**: [8, 16, 24, 32] unidades\n",
        "* **dropout** nas camadas densas: entre 0 e 0.5"
      ]
    },
    {
      "metadata": {
        "id": "7nEddPYZYKVY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "conv1 = [32, 48, 64, 96]\n",
        "dense1 = [64, 96, 128]\n",
        "dense2 = [16, 24, 32]\n",
        "    \n",
        "def model(X_train, Y_train, X_test, Y_test):\n",
        "    conv1 = {{choice([32, 48, 64, 96])}}\n",
        "    dense1 = {{choice([32, 64, 128])}}\n",
        "    dense2 = {{choice([16, 24, 32])}}\n",
        "    \n",
        "    input_layer = Input(X_train.shape[1:5])\n",
        "    \n",
        "    conv_layer1 = Conv3D(conv1, kernel_size=(3, 3, 3), activation='relu')(input_layer)\n",
        "    pooling_layer1 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer1)\n",
        "    \n",
        "    flatten_layer = Flatten()(pooling_layer1)\n",
        "    \n",
        "    dense_layer1 = Dense(dense1, activation='relu')(flatten_layer)\n",
        "    dense_layer1 = Dropout({{uniform(0, .5)}})(dense_layer1)\n",
        "    \n",
        "    dense_layer2 = Dense(dense2, activation='relu')(dense_layer1)\n",
        "    dense_layer2 = Dropout({{uniform(0, .5)}})(dense_layer2)\n",
        "    \n",
        "    output_layer = Dense(units=1, activation='sigmoid')(dense_layer2)\n",
        "    \n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    \n",
        "    opt = optimizers.RMSprop(lr=0.0001)\n",
        "    \n",
        "    model.compile(loss=binary_crossentropy, optimizer=opt, metrics=['acc'])\n",
        "\n",
        "    #early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=1, mode='auto')\n",
        "    \n",
        "    model.fit(X_train, Y_train,\n",
        "              batch_size=128,\n",
        "              epochs=10,\n",
        "              verbose=2,\n",
        "              validation_data=(X_test, Y_test)#,\n",
        "              #callbacks=[early_stop]\n",
        "              )\n",
        "    \n",
        "    score, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
        "    \n",
        "    print('Test accuracy:', acc)\n",
        "    \n",
        "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VmC4-VLSYKVc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Otimização\n",
        "\n",
        "Os dados são lidos e é feito o processo de otimização com o Hyperas. São testados 50 modelos."
      ]
    },
    {
      "metadata": {
        "id": "s1S6hLShYKVd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, Y_train, X_test, Y_test = data()\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "best_run, best_model = optim.minimize(model=model,\n",
        "                                      data=data,\n",
        "                                      algo=tpe.suggest,\n",
        "                                      max_evals=30,\n",
        "                                      trials=Trials(),\n",
        "                                      verbose=False,\n",
        "                                      notebook_name='drive/My Drive/Pesquisa - Dicom images/notebooks/optimizer_5_balanced_repeat')\n",
        "end = time.time()\n",
        "\n",
        "print(\"Tempo para otimização:\", (end - start)/60, \"minutos\")\n",
        "\n",
        "\n",
        "print('Summary of the best model: -----------------------')\n",
        "\n",
        "print(\"Evalutation of best performing model:\")\n",
        "print(best_model.evaluate(X_test, Y_test))\n",
        "\n",
        "print(best_model.summary())\n",
        "\n",
        "c1 = conv1[best_run['conv1']]\n",
        "d1 = dense1[best_run['dense1']]\n",
        "d2 = dense2[best_run['dense2']]\n",
        "drop1 = best_run['Dropout']\n",
        "drop2 = best_run['Dropout_1']\n",
        "\n",
        "print('Conv 1:', c1, 'unidades')\n",
        "print('Dense 1:', d1, 'unidades')\n",
        "print('Dense 2:', d2, 'unidades')\n",
        "print('Dropout 1:', drop1)\n",
        "print('Dropout 2:', drop2)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}